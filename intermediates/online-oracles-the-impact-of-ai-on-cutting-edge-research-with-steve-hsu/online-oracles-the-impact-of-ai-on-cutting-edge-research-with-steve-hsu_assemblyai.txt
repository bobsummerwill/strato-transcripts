SPEAKER_00: Okay everyone, welcome to this very special episode of Onchain Oracles. We are going to not cover crypto today, but cover something that I think is very interesting. Our special guest today is Steve Hsu. He is a theoretical physicist and founder of several startups primarily on AI and genomics.

SPEAKER_01: Asien, how are you guys? Great to see you.

SPEAKER_00: We are very happy. Also, I want your help to correct the record because in our year review episode I mentioned one of the exciting developments was really in AI. I mentioned that your paper that got published that you talked about getting AI assistant, but I made a mistake and I think I said it was co authored by AI, but we'd like to have, you know, that clarity on what actually happened. And of course with us we have our usual host, Bob Summerwell, head of ecosystem.

SPEAKER_01: Hey there.

SPEAKER_02: And. And we've got some blurry Kieran and Jims.

SPEAKER_03: Yes, even closer.

SPEAKER_00: Yes.

SPEAKER_03: Confused with the, the backlit. We are in the same location but in separate rooms and so neither setup is completely optimal. But we're here.

SPEAKER_02: Maybe don't blur at all.

SPEAKER_01: Yeah, maybe don't blur.

SPEAKER_03: Do we know how to turn it.

SPEAKER_01: Off or put the office potted plant background in?

SPEAKER_00: Yes, exactly, exactly. So, so why don't we dive right into it? Steve, can you talk about, can you give us a summary of the papers and how AI was used in the creation of papers?

SPEAKER_01: So for listeners who didn't follow all this, and there's, there's sort of different smatterings of this whether you watched what happened on X or on various people's substacks or on YouTube. So, so I've been working for the last, most of a year now in evaluating essentially all the frontier models and how good they are at theoretical physics and to a lesser extent pure math. And on my own podcast, manifold, there are some episodes about this. There's even an episode with one of the teams that was able to get international Math Olympiad gold performance, gold level performance from off the shelf models. So I've been quite interested in this question of how useful are the models for frontier level work or superhuman intelligence level work in math and physics? And so I've been working on that in testing the models. I would. Often one of the problems with dealing with them, as you guys know, is that they hallucinate. And so it's best to test them in a domain that you really know well, that you know in a super deep level. Otherwise they'll say things very confidently that might just be totally untrue and it might slip past you. So one of the ways that I test the models is, you know, having been a researcher now for many years and having published, gosh, I don't know, the number might be approaching 150 research articles. I often will ask them. I'll upload one of my old papers and I'll ask the model about it. I'll ask the model to explain it to me. I'll ask the model about other directions in which the research could be continued, in which maybe I and my collaborators thought about it, but we didn't actually publish some of those results. So there are ways to test its deep understanding, but there are also even ways to test it on things that clearly are not in the literature, that it cannot have memorized from some training source, etc. So it's a very fruitful way to test the models. And in one case I was testing its understanding of a paper I wrote about 10 years ago, which I actually based on the interactions we had with multiple referees at the time, ultimately the paper was published, but we had lots of interactions with referees along the way. I could tell the community didn't really understand this topic very well. And so I was quite interested to see how well the models could understand it. GPT5, I was testing GPT5. It understood the model, I mean the paper extremely well. And actually it made a suggestion for a continuation of the research which I found super interesting. I had not never seen that before what it proposed. And so then I started investigating its proposal. Now its proposal came in the form of a very long answer which included derivation of a bunch of new equations which I had never seen before. And I then further, I used GBD5, but then other models as well to then investigate that line of research which turned out to be completely novel and quite interesting. And I wrote a paper based on those results.

SPEAKER_00: Okay, and do you like. I know you wrote a companion piece to that paper also how you used AI through the entire process, was that always your intention to.

SPEAKER_01: Yeah, well, I should also say that during this period of time I've been working with a team at DeepMind and that team built a specialized AI called CO Scientist. And Co Scientist is meant to be a kind of research assistant or research collaborator for scientists. And I had been doing a lot of testing, trying similar types of things with CO scientists. It just happened that this particular novel idea came from during my testing of GPT5. And so I, I think GPT5, of all the models involved deserves the most credit. But I had been doing this kind of thing with the Co Scientist team. Now as I was developing the research, I always intended to publish it if it turned out to be good. If it turned out this set of equations was correct and did something useful and was worth thinking about. So I was always thinking, oh, if true, this would be the first example of the model actually suggesting out of the blue, completely de novo, some new idea in theoretical physics which, which actually really did happen. And I thought that by itself would be remarkable. So I was thinking about doing that and I was actually Talking with the DeepMind co scientist team about this because we were talking about stuff that co scientist was doing for us. But I also mentioned to them very early on I said look what I got from GPT5. This is like super interesting and maybe this will turn into like the ultimate goal of the co scientist work would also have been to produce a publishable novel result which we would then publish. Okay, so, so that was the general thing that we were attempting to do. It just happened to originate in this case from GPT5. So I was planning to do that. Now what's the way you can show the community that it's non trivial? Because I don't want it to rely on my subjective judgment. The way to do that is to get the paper refereed and published in a reasonable journal, in this case a good journal, Physics Letters B. Now if you submit the paper and you say, hey, this whole paper is written by AI or the ideas in this paper all came from AI it's not going to be reviewed in the standard way because people could react emotionally or they don't like AI or they'll just assume it's AI slop, or maybe they'll be the other way and they like AI so they'll treat it in a more favorable way. For whatever reason, I wanted to avoid that. So when I submitted the paper I filled out the AI declaration. There's now a required AI declaration for most scientific journals. And I said, yes, I use these models in the preparation of this paper. But you're not required to declare something like oh, the original idea for this, the whole paper came from an A. That's not something you're required to declare. So I did not declare that because I didn't want to bias the process. Okay, So I believe I got a completely unbiased evaluation of the paper from the referee and the editor at the journal. I don't know who the referee was. I do know who the editor was. Editor's a very well known string theorist in the uk. So I think I got a fair review. I think the paper fully deserves to be published and has novel results. I have since interacted very strongly with two other theoretical physicists who evaluated the paper and commented on it online. One is a guy at iit, a professor at iit, and another is a guy called Jonathan Oppenheim, who's a professor at University College London, who, the second guy I've known for, I don't know, 15, 20 years. 15 years now. So if you're interested at all in the physics. Well, both AI and the physics, but the three of us have done a YouTube podcast discussion together which is almost 90 minutes long and we discuss all this stuff and I think we're pretty close in our opinions now after all of this discussion, you know, and I think we all agree it is worth publishing. There are interesting results, novel results. It's not AI Slop. We might differ on some very fine tuned evaluations of certain aspects of the results and of the paper, but generally we agree. And so whatever you read online, whatever junk you, you read online about this paper, I would say forget about that and go watch the 90 minutes or read the transcript of the 90 minutes of our discussion that's just sitting there on YouTube. Otherwise you have no basis for opinion. Okay, These are the three most qualified people in the world to judge this because they spent the time reading the paper and we've exchanged lots of tons of email and then spent 90 minutes discussing it so that that's the place to go for an evaluation of the quality of the work.

SPEAKER_00: It sounds like, in effect, you almost created like a Turing test for scientific validity of an AI generated idea.

SPEAKER_01: Well, aside from all the. Okay, so some people, I think actually both Nirmalia and Jonathan were predisposed to calling it AI Slop. Like if you look at the first things they wrote on their substack, they were pretty negative. And, and so we had to have back and forths about all this stuff to clarify things. There's a. I was surprised by this actually, that there's a significant chunk of the scientific community, even the physics, theoretical physics community, which I thought would have been pretty rational about this, that were predisposed toward negativity, toward viewing this as AI slob. So I didn't actually think I was taking any reputational risk because I just thought, oh, I'm going to go through this, I am going through this in good faith with the DeepMind team.

SPEAKER_00: Right?

SPEAKER_01: And I'm doing this in good faith. And then we, I've discussed this with the DeepMind team. I said, you know, when we get something that's really good. We want to submit it to a journal, but we want to submit it without declaring that it is sort of 100% the work of an AI. We want to just get it refereed and evaluated the way like an ordinary paper of mine. That, that was sort of our goal. And then we, we always agreed we would, at the moment the paper's accepted and we posted on the archive we would acknowledge the exact role of AI in producing paper, which I did. So there's a, there's a companion paper to my main paper which is actually has literally the prompts and responses from the model. And so, and so there's no hiding anything about the process that produced paper. But it's best to show that at the end. Like if you show that at the beginning there's enough anti AI bias that it would, would distort the process.

SPEAKER_00: I, I'm, I'm curious like, given that reaction and I've seen that because of, even when I misspoke about what you had done with the paper, that got leveraged to be ammunition against the paper. Even though that was not your mistake in any way that, you know, like in the scientific community does. Do you think that makes it much harder to evaluate the benefits or, you know, weaknesses of AI in cutting edge research like you're.

SPEAKER_01: I think it does. I, Well, I experienced that myself firsthand because I just feel like there was an initial wash of negative, by negative bias. There was initial negative bias that I was just actually not expecting. Maybe it's because I also do research in AI and I'm involved in AI startups. So maybe I just assume people had this naturally kind of open attitude toward, you know, we're doing evals of AIs all the time, so we want to know how good are these things? Like initially, just one year ago when I was testing the models, their understanding of deeper physics and advanced math was pretty weak. And so the only way you can know that the labs are actually accomplishing their goal of improving is just constant testing and eventually you need expert testing. Right. So I viewed all of this as just a part of that natural participation in the AI research process, the frontier AI, you know, evaluation. And I was surprised that there's a subset of the physics community that just saying, oh, this is going to be slop, this is slop, this is terrible, et cetera, et cetera. So, yeah, very strange. Now I think that what this shows though is that there, this, this is in a way a historic first example. So this is the first published theoretical physics paper in which the main idea came from an AI. I think that's just flatly true. Unless there's some secret paper other people have published, they just haven't revealed that this is true of. But given my careful sort of monitoring of the model progress in the last year, it's unlikely that that happened much before when I, when it happened for me, because I was doing this very, you know, I was very engaged in doing this kind of thing. So this might be the first paper in which an AI, it had some idea. It's not like Einstein's theory paper on special relativity, it's nothing like that. But it did connect to previously unconnected areas of physics and make some observation and derive some unique equations. I should mention that there's a paper in this subject by two authors at Johns Hopkins University, Kaplan and Regendaran. Neither of them were involved in this, although they were involved in some of the correspondence between the three of us later on. But they have written pretty big papers in the last few years on exactly this topic. And the new equations proposed by the model by the AI GPT5 show that their work violates relativistic covariance. So. So there is a non trivial outcome that affects the active research of some leading theoretical physicists who were not involved in this at all, who don't use AI at all, as far as I know, or didn't use AI in their work. So the point is, said something non trivial which the referees of their papers had not noticed. Okay. And I think Jonathan and Nirmalia both acknowledge this. They agree that what the model calculated about the KR work, Kaplan Regentron work was correct and showed that the Kaplan Regent work has some issues. Right, Right. So there's clearly non trivial stuff happening here. It's just too bad that the way the general Internet world perceives this is through some kind of like AI good AI bad kind of lens.

SPEAKER_00: Oh, sorry, go ahead.

SPEAKER_03: Yeah, so I had the experience of just using the models kind of casually sometimes for like marketing copy and you know, it's varied on that. Sometimes quite good, sometimes not, and then generating code snippets or just like asking questions. And I remember very clearly I had the experience of the AI being able to write new code but not able to ingest an existing code base and it switched to oh, it can actually read our code base much faster than any human. And I can answer questions that I had about the code base that I never got a human answer for, maybe because I never asked or, you know, it's remarkably coherent in its understanding. It felt very overnight. But also there's a skill element and I'm guessing in part, you know, you work with the models for a long period of time, you kind of learn how to get the best output out of them. It's, it's sort of like writing Google searches or SQL queries or something. It's like you got to interact with it a certain way to get the output you want. And I think you covered some of that in your paper. Do you want to tell everybody?

SPEAKER_01: Yeah, that's a great point, Kieran. This is a very important aspect of the discussion that we haven't touched on yet, but it's very central to the main point. I think that would be useful to most listeners, which is that I didn't just use any process to get these results. Okay? So when I mentioned the researcher, he's a UCLA CS professor that I interviewed who his team had gotten gold medal IMO performance from off the shelf models, they didn't just do it by making single shot one shot queries of the models, they built a whole pipeline which their terminology, and my terminology is the generator verifier architecture where you chain together or even chain parallel streams of instances of the model or one of many models which are trying to solve the problem and proposing a solution, but then other instances that are critically evaluating what has been previously produced, finding problems with it, making suggestions and then you iterate. And that process, in the case of the brand new most recent IMO problems, produced 5 out of 6 correct proofs, whereas one shot attempts with any of the top models at the time on IMO problems only got maybe one out of six proofs. So you can see there's in a sense almost an order of magnitude improvement of the performance if you're willing to expend 10x or more tokens and also chain together differently prompted instances of the model or even different models, which I did in my generator verifier chain I was using multiple different models. I was using GPT5, I was using Gemini, I was using Quinn. That is a process which very few people have actually experienced. It's different from, oh, I put it into deep think mode or I use the most recent Claude, but I'm always doing one shot. Okay, I might do a lot of reasoning, but I'm just doing one shot. Well, try chaining that together many times and don't look at any of the output till the end. And at the end you might find, oh, I have a perfect proof of problem 4 on the most recent IMO. Whereas the one shot thing that was produced is just junk, right? So anybody who says AI stuff is slop, I have to ask them, well, what are you talking about? Are you talking about the one shot product is slop or are you talking about something that at the end of a pipeline like I just described is sloped? Because those are two very different statements. And the set of people that are able to make a qualified statement about the second thing need to be one, an area expert in some very deep area. And number two, have actually built that architecture and used it. So it's almost a subset, it's almost a measure of like almost zero. Now, now co scientist does do this. Co scientist is built to have this kind of chain of generator verifier type things in it. My own process has it, the UCLA thing, which is actually on GitHub. So if you want to prove some IMO, if you want to solve some platinum or IMO problems, you can go download like their, their GitHub repository and work. But the point is it's a very small set of people that have seen the maximum level of model capability. Most people have not seen the maximum level of current model capability. No.

SPEAKER_02: And it can be quite time consuming to, to build those pipelines and they can be quite slow as well. It is quite a difference from. Here's hardly any context. Just magically give me the answer.

SPEAKER_01: Yeah, so in the case where you have the pipeline built and you don't look in, then it's a very different experience because you don't experience the bad one shot results. You put something in and then of course it may take a lot more time than you're used to, but then if you just look at that output, you'll think that you're dealing with a significantly smarter intelligence than otherwise.

SPEAKER_04: You know, I just want to point out this is why the singularity is near, because you can spin up a team of adversarial collaborators with a button press.

SPEAKER_01: Yeah, no, it's so, so that's my point. So in the companion paper that I wrote to my physics paper, this was the point I was making, is that you should use a generator verifier pipeline. You can get qualitatively better results. And it's a legitimate question, like given the base models that people have produced, what's the right way to get the best outcome? Is there a way to chain them together in a way that enhances the outcome qualitatively? And I would claim the answer is yes.

SPEAKER_00: Do you have any sense of why this works and which different LLMs should be combined in a different way or is it just simply getting one to check another?

SPEAKER_04: Isn't it just that like, like if you have a room of people arguing, right?

SPEAKER_03: Like if one is like way dumber than the other, it might throw a bunch of bad critiques in or something. It's interesting. Yeah, it's a good point.

SPEAKER_01: Let me, let me give you the empirical. Let me, I'll give you the empirical results and then I'll give you some, some theoretical hypotheses. So the empirical results are. The UCLA team in the IMO context started with Gemini and then by chaining together the generator verifier instances, they were able to get Gemini to solve five out of six problems. They subsequently wrote a follow up to their paper or an amendment to their original paper in which they showed they also had gotten the latest GPT and grok and I forgot one more. At least one more. So they've gotten multiple commercial off the shelf models by using this process. So just instances differently prompted of the same commercial model, but chained together, they had gotten all of those pipelines to the 5 out of 6 level. The 6 problem on the IMO was strangely hard compared to the other 5. Currently the best performance anybody's gotten is 5 out of 6 on these things. So that's an empirical result. My empirical result is I was testing the models first in one shot mode. And I had concluded at the time, this is a few months out of date now, but maybe six months ago the best models for theoretical physics for me, for what I was asking about were GPT5, Gemini and Quinn Max. Those were the three best. And deep seq. Just a little bit behind was Deep Seek and then maybe behind was grok. So there's variation in how good the models are at that time. You know, that snapshot, there was some variation in the quality of the models. Okay, so that's the empirical result. Now there two hyp. There's some hypothetical, some hypotheses I could, I could propose. One is that, imagine that you have models that have an X percent chance of detecting an error. Okay, so maybe they have a 60% chance of detecting an error in the previous output that it's looking at, it's critically examining. Well, if you chain together enough like you might, you know, you might have an increased chance of eventually catching all or almost all the errors in, in what's flowing through the pipeline. Okay, so, so I think that's a, not an unreasonable model for what's happening. So as long as the model instances that you're sticking in there have a reasonable chance of detecting any particular error in a proof or in a physics argument. Like okay, it could be that it's adding value, maybe at some point it's not, you know, it's not able to fully beat down the level of errors to zero, but it's, it's improving the result quite a bit. I think that's a reasonable, reasonable hypothesis. Now one thing that I noticed in dealing with lots of models and doing one shot stuff with lots of different models is they fail in different ways. They make different mistakes. All these models have been trained differently. Different pre training, different rl, different reasoning stuff. Reasoning step like ways of causing it to do reasoning or structuring its reasoning. So they all different ways and so by chaining together very independent instances where one's Quinn and one's deep seek and what you know, I feel like you have sort of not completely correlated things in there, each of which have an independent chance of catching every error. So you're sort of getting a robust error suppression pipeline going. That's how I view what all this is.

SPEAKER_04: I liked Kieran's comment before though. And now I'm going to make a prediction. We are going to see groupthink and bubbles within these collaborators in the future. I'm not saying it's a bad strategy, but I'll bet you that we will.

SPEAKER_03: See that maybe less if you mix the models it sounds like.

SPEAKER_01: But yeah, I mean if you mix the models the models are very different like the Chinese models they have. You know, if you imagine this education system for 1.4 billion people in which they do a lot of testing, they have huge numbers of solved problem sets, solved problems in the Chinese educational system through college and grad school that aren't necessarily in the corpus that say anthropic or OpenAI are using to train their model. So there is some genuine independence in the detailed capabilities of the models. You don't see this so much in benchmarks because a benchmark is just one, just giant average score. But imagine you meet like a grad student came from Bulgaria and he had done all these Soviet era problems and then you meet another grad student who went to ucla. Well those two guys are going to have different gaps in their knowledge, right? So it's the same, it's the same kind of thing.

SPEAKER_03: Do you ever see error blow up like, like Jim was saying, like they could theoretically reinforce an error versus.

SPEAKER_01: Don'T, but you can have. So, so one of the things I comment on in my companion paper is that there were. So the fact that all the models converge doesn't mean that the answer is correct, but it means that there's a, there's a, you know, higher chance that it's correct. Right. Than if they hadn't converged. When they fail to converge, which is, one model says, no, this is how it goes. And the other model says, wait, that's wrong for this reason. And then you show it to another model and if they just oscillate, I'm pretty like, in my experience, that's a pretty good signal that something's wrong. That, that just generally like the models are not really understanding this well or, or calculating right about this. So. So that did happen in some crucial aspects of the research. And so there I had to really drill down myself and try to figure out what was really going on.

SPEAKER_04: But you, you have seen even within academic humans, bubbles form too.

SPEAKER_01: So. Yeah. Well, do you mean bubbles or cycles where people just don't. Bubbles?

SPEAKER_04: I. I don't know if I want to put you on the record, but there are certain topics in, in physics.

SPEAKER_01: Which I think you have stated. Oh, I see what you're saying. Okay. So you could have a case where everybody agrees and they're all wrong. Yeah. Yeah. So. And they're all magnets. Yeah, that can happen with. I'm sure that can happen with the models. And it happens.

SPEAKER_04: A is like, kill the humans. And B is like, okay, kill the humans. Hey, yeah, kill the humans.

SPEAKER_01: Well, remember, you're prompting the verifier instance to be extreme. It's. It's predisposed to be extremely critical of every. That's important. Yeah. Again, like, for people who only have one shot experience with models, they're dealing with a one shot instance that has been rl to be a friendly AI. The AI never says like you're. You're an effing idiot, you know, and all so. But the thing is like, you can create instances by proper prompting of models that are much more critical and much more predisposed to saying something's wrong. Right. And so by chaining that together, you get a, you get something.

SPEAKER_04: It is more the referee and a researcher dynamic.

SPEAKER_00: So.

SPEAKER_01: Exactly.

SPEAKER_00: I've experienced this by telling the model like, hey, when you look at this output from another model that it was made by a junior developer or there are obvious mistakes in it. What are they? And that seems to be a better.

SPEAKER_01: Yeah, you can totally give the verifier instance in the pipeline the prior that there are errors. Right. So, you know, I mean, again, this is not directed at you guys. But for you people out there who are like, AI slop. AI slop. Yes, I'm sure you have seen AI slope, but have you seen the best thing that a determined person can get out of these models? You may not have.

SPEAKER_00: So it makes sense that there is a. Well, I think you highlighted two things that are kind of crucially important is that to. That there is a skill in doing this right, even if you follow this process, but also that you need to be a bit skeptical about the output and you need to kind of look at that output kind of probabilistically. Even if it's gone through this process, you gotta say, oh, wait, like, is this. Actually I want to verify it before I just assume it's correct?

SPEAKER_01: Yeah, absolutely, Victor. I mean, I'm only saying that the models are useful to experienced researchers. I make a very specific warning in the companion paper. I say even a PhD, like a senior PhD student or recent PhD, if relying too much on models could produce lots of wrong subtly. I say in the paper, subtly wrong results because that person doesn't have this huge depth of experience and could be fooled by the model outputs. The positive statement I'm making is that if you take an experienced researcher, that person's productivity can be enhanced using models the way I describe. So that's a very minimal statement. It's not like, oh, models have replaced physicists or, or whatever. It just definitely appears to be useful. Now, both Jonathan and Nirmalia, if you listen to our discussion, both agree with me that they find the models useful in their research. And I think a lot of the naysayers don't, without listening to that conversation, would not accept that. They would just think, oh, this. Even some other physicists who haven't really experimented much with the models would say things like, oh, it's just going to waste your time. It's all crap. They don't really understand physics. But nobody, I think, who is seriously using the models and trying to do research has that opinion anymore.

SPEAKER_00: I. I'm curious on two things. One is, like, how much Steve said.

SPEAKER_03: He had a hard stop. Oh, yeah, yeah, I can be late.

SPEAKER_01: I can be a little late for them. So, like, maybe we got four more minutes or five more minutes.

SPEAKER_03: Four more minutes. Okay.

SPEAKER_00: You know, like, how much time, you know, given that you are like the first person to do this, I'm sure using the mottos was more cumbersome than a normal person, but do you. What do you think the time savings was by using this motto that you experienced or the inefficiency, was it a 2x factor, 10x factor? You know, how do you think about that?

SPEAKER_01: The proposal of that set of equations I probably never would have done myself. Okay, so that was just a weird leap. Now again, it's not a leap like Einstein jumping to special relativity but, but, but it was a non trivial delta leap that I probably wouldn't have made on my own just because it wasn't. It combined some stuff that I had not really thought that much about myself. But put that aside, that's kind of a one off thing. And just like ask the further refinements, calculations I did for the paper, typesetting the equations in tech, all that stuff. It definitely saved me weeks of work. There's no question it saved me, you know, definitely like a 2, at least a 2x speed up in how long it took to, to complete the paper and.

SPEAKER_00: Oh, sorry. Go ahead Karen.

SPEAKER_01: Okay.

SPEAKER_03: I wanted to get this one.

SPEAKER_00: Okay, last question.

SPEAKER_03: Day to day use like let's say maybe even not in your formal research capacity like we were talking today. I've never booked a flight with an AI agent. I've had a. Jim has had a route plan. I've kind of route plan, had it plan itineraries. Do you have any tips for more, you know, I don't know, physics, layman use cases or just things in your, your life that we might not have figured out how to do with AI?

SPEAKER_01: Yeah, so another great question, Kieran. So, so if you think about what I'm saying with this like generator verifier pipeline, blah blah blah, that's just a very special constrained instance of an agentic pipeline, right? Because like I have a, I have two types of agents gener, you know, really you have generator verifier and then modifier verifier. Right. So. So you have you know, basically a small set of types of agents in there and they're all sort of doing stuff, pushing, pushing stuff through, right? And you could say like oh, could I have an agentic pipeline that books airplane flights with me? And one of them just knows that like I always want an aisle seat and it's just demanding all the time, like does a reservation and the other one's like Kieran hates flying on Spirit Airlines. Don't ever book. You know, you could you chain together some agents and then like profitably have the thing book reasonable airline reservations free maybe. I haven't tried it. I know there are a ton of companies trying this right now, you know, generating a proof for a mathematical hypothesis or Coming up with a physics, you know, correct physics that's a very constrained problem and the model's all kind of agree with each other on what's quote, right or wrong. So it's a, it's a more constrained problem than like buy me the best fleece puffy jacket from Amazon. Right. That's like. So, so I'm not saying that we, we can solve all those problems right now with agents like the, the big question of how. Well agent identified pipelines, whether it's encoding or purchasing stuff on the Internet, the rate at which those improve over the next year or two is a very open question and actually plays a big role in whether the AI bubble is going to, you know, generate real value from. For billions of people sooner rather than later. I don't have an answer for that. But for the very specific thing of like is it useful for professional mathematician or physicist to use this? I would say the answer is definitely yes.

SPEAKER_04: But by the way, I just want to put on the record here, it might sound like we're being like devil's advocate a bit here, but I know everybody in this call and 100% of them are super open users of AI for everything right now. Maybe, maybe not everything but, but for, for, for work related tasks. So we're all very pro.

SPEAKER_01: Yeah.

SPEAKER_03: Well figure out how to better instruments. The verifier pipeline. I do it some. I also do it in just like repeat threads. I try to like tell the AI it's wrong and then you know, see how it comes and sometimes it agrees with me. You know, it's just like a long discussion thread is like a human verifier AI generator. But, but yeah, I, I feel I'm still. There's a lot there that we could if we were just better at it. That would be one of the things.

SPEAKER_01: I'm working on is a, I mean the, the next phase of this work, this project is to build a pipeline like the kind I've described but for general usage of the whole physics community and eventually, you know, chemistry or math or whatever, whoever wants it and in which that, that all that stuff is hidden. And yeah, you're maybe burning through lots of tokens and it's expensive. But on the other hand the data that you get back from the way the researcher interacts with this super AI is valuable to the lab. So I'm actually trying to get that built. So I think you're going to see things like this where the token budget is large and there are many quote agents or generators and verifiers in there working together, and I think that'll be, like, in the hands of researchers in the next year or two for sure.

SPEAKER_00: That that's incredible. So, on that, I think we'll wrap up. I know you. You have another appointment to attend to, but thank you so much for your time, as always. Where can people go to if they want to hear more of your thinking on this? AI on?

SPEAKER_01: You know, if you're not already tired of. If you're not already tired of me, go on X and you'll see a lot more of me. My X speed. And I have a podcast called Manifold that every two weeks, I release an episode. Usually I'm interviewing somebody else.

SPEAKER_00: Awesome. And we'll make sure to include links of this in our notes. Thanks again, Steve. Take care.

SPEAKER_01: All right, guys, make Crypto great again.

