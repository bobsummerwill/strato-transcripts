**[00:03] SPEAKER_00:** Okay, everyone, welcome to this very special episode of On-Chain Oracles. We are going to not cover crypto today but cover something that I think is very interesting. Our special guest today is Steve Hsu. He is a theoretical physicist and founder of several startups, primarily on AI and genomics. Hey, Steve, how are you?

**[00:24] SPEAKER_04:** Great to see you guys.

**[00:25] SPEAKER_00:** We are very happy. And also, I want your help to correct the record because in our year-end review episode, I mentioned one of the exciting developments was really in AI. I mentioned that your paper that got published, that you talked about getting AI assistance, but I made a mistake and I think I said it was co-authored by AI. But we'd like to have that clarity on what actually happened. And of course, with us, we have our usual host, Bob Summerwill, head of ecosystem.

**[01:02] SPEAKER_01:** Hi there. And we've got some blurry Kieren and Jim.

**[01:05] SPEAKER_00:** Yes.

**[01:06] SPEAKER_02:** Even closer. Yes. I'm confused with the backlight. We are in the same location, but in separate rooms. And so neither setup is completely optimal. But we're here.

**[01:17] SPEAKER_01:** Maybe don't blur at all.

**[01:19] SPEAKER_02:** Yeah, maybe don't blur. Do we know how to turn it off?

**[01:23] SPEAKER_04:** Or put the office potted plant background in.

**[01:26] SPEAKER_00:** Yes, exactly, exactly. So let's dive right into it. Steve, can you talk about, can you give us a summary of the papers and how AI was used in the creation of these papers?

**[01:37] SPEAKER_04:** So for listeners who didn't follow all of this, and there's sort of different smatterings of this, whether you watched what happened on X or on various people's Substacks or on YouTube. I've been working for the last most of a year now in evaluating essentially all the frontier models in how good they are at theoretical physics and to a lesser extent, pure math. And on my own podcast, Manifold, there are some episodes about this. There's even an episode with one of the teams that was able to get International Math Olympiad gold performance, gold level performance from off-the-shelf models. So I've been quite interested in this question of how useful are the models for frontier level work or superhuman intelligence level work in math and physics. And so I've been working on that. In testing the models, one of the problems with dealing with them, as you guys know, is that they hallucinate. And so it's best to test them in a domain that you really know well, that you know in a super deep level. Otherwise, they'll say things very confidently that might just be totally untrue and it might slip past you.

So one of the ways that I test the models is having been a researcher now for many years and having published, gosh, I don't know, the number might be approaching 150 research articles. I often will ask them, I'll upload one of my old papers and I'll ask the model about it. I'll ask the model to explain it to me. I'll ask the model about other directions in which the research could be continued, in which maybe I and my collaborators thought about it, but we didn't actually publish some of those results. So there are ways to test its deep understanding, but there are also even ways to test it on things that clearly are not in the literature, that it cannot have memorized from some training source, et cetera. So it's a very fruitful way to test the models.

And in one case, I was testing its understanding of a paper I wrote about 10 years ago. Which I actually, based on the interactions we had with multiple referees at the time, ultimately the paper was published, but we had lots of interactions with referees along the way. I could tell the community didn't really understand this topic very well. And so I was quite interested to